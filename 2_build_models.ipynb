{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config Variables\n",
    "\n",
    "# Define UCI Datasets to pull\n",
    "# https://archive.ics.uci.edu/ml/datasets.php\n",
    "uci_datasets = [1,9,10,16,60,87,89,92,162,165,186,189,211,242,244,270,275,291,294,300,320,332,368,374,381,390,409,464,471,477,492,519,544,547,560,563,565,597,601,713,849,851,857,880,890,925,942]\n",
    "#183,\n",
    "\n",
    "# List of file names provided in the R code from 2018 Half Ridge Paper\n",
    "paper2018_datasets = [\n",
    "    \"house.world\", \"mortality\", \"cit.world\", \"prf.world\", \"bodyfat.world\", \n",
    "    \"car.world\", \"cloud\", \"dropout\", \"fat.world\", \"fuel.world\", \"glps\",\n",
    "    \"homeless.world\", \"landrent.world\", \"mammal.world\", \"oxidants\",\n",
    "    \"attractiveness.men\", \"attractiveness.women\", \"fish.fertility\", \n",
    "    \"oxygen\", \"ozone\"\n",
    "]\n",
    "\n",
    "# Use label encoding or one hot encoding for categorical variables? values: 'label', 'onehot', 'none'\n",
    "categorical_encoding = 'onehot'\n",
    "\n",
    "# How to handle missing data? values: 'mean', 'median', 'mode', 'drop', 'zero', 'ffill', 'bfill', 'interpolate'\n",
    "missing_data_handling = 'mean'\n",
    "\n",
    "# Use all data (vs just training data) for OLS to determine co-efficient sign\n",
    "ols_use_all_data = True\n",
    "\n",
    "# Training set sizes to use (only training set sizes < 60% of total dataset size will be used)\n",
    "#training_set_sizes = [10, 40, 160, 640, 2560, 10240, 40960]\n",
    "training_set_sizes = [10, 40, 160, 610, 2560, 10240, 40960]\n",
    "chain_lengths = [20000, 20000, 100000, 100000, 100000, 100000, 100000]\n",
    "\n",
    "# Number of etas (prior standard deviation) to test between 1e-04 and 1e05 (recommend 20)\n",
    "# Limits at 0 and infinity are calculated separately\n",
    "# Note: In practice, sigma^2 (likelihood variance) is set to 1/eta^2 and\n",
    "#       lambda (penalty) is set to 1/eta^4 \n",
    "num_etas = 20\n",
    "\n",
    "# Number of folds for cross-validation (set to 300 in 2018 paper, recommend testing with 20)\n",
    "cv_folds = 20\n",
    "\n",
    "# Overwrite model if it's already saved to file\n",
    "overwrite_model = False\n",
    "\n",
    "# Debugging Comments\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries and helper functions\n",
    "\n",
    "# import half_ridge functions from helpers/half_ridge.py\n",
    "from helpers.half_ridge import *\n",
    "\n",
    "# import testing functions from helpers/testing_functions.py\n",
    "from helpers.testing_functions import *\n",
    "\n",
    "# import dataset class\n",
    "from helpers.model import Model\n",
    "\n",
    "# import os and pickle\n",
    "import os\n",
    "import pickle\n",
    " \n",
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists for dataset 1. Skipping...\n",
      "Model already exists for dataset 9. Skipping...\n",
      "Model already exists for dataset 10. Skipping...\n",
      "Model already exists for dataset 16. Skipping...\n",
      "Model already exists for dataset 60. Skipping...\n",
      "Model already exists for dataset 87. Skipping...\n",
      "Model already exists for dataset 89. Skipping...\n",
      "Model already exists for dataset 89. Skipping...\n",
      "Model already exists for dataset 89. Skipping...\n",
      "Model already exists for dataset 92. Skipping...\n",
      "Model already exists for dataset 162. Skipping...\n",
      "Model already exists for dataset 165. Skipping...\n",
      "Training Set Value: 10 for dataset 186\n",
      "   Training Set 10 full run took 1.1920928955078125e-05 seconds\n",
      "Training Set Value: 40 for dataset 186\n"
     ]
    }
   ],
   "source": [
    "# Load the UCI dataset, normalize the data, and split into training, validation, and test sets\n",
    "\n",
    "# Loop over each dataset, loaded from file and formatted\n",
    "for dataset in uci_datasets:\n",
    "    model = Model()\n",
    "    target = 0\n",
    "\n",
    "    # iterate over the target (while the file exists)\n",
    "    while os.path.isfile('uci_ml_datasets/ucirepo_' + str(dataset) + '_' + str(target) + '.csv'):\n",
    "\n",
    "        model.import_from_file('uci_ml_datasets/ucirepo_' + str(dataset) + '_' + str(target) + '.csv')\n",
    "        model.format_data(categorical_encoding, missing_data_handling)\n",
    "        training_set_values = model.set_up_training_values(training_set_sizes, .6)\n",
    "\n",
    "\n",
    "        # Check if a file exists for the dataset in the model_results folder\n",
    "        if os.path.isfile(f\"model_results/ucirepo_{dataset}_{target}.csv\") and not overwrite_model:\n",
    "            print(f\"Model already exists for dataset {dataset}. Skipping...\")\n",
    "            # increment the target\n",
    "            target += 1\n",
    "            continue\n",
    "\n",
    "        # Print the training set values for this dataset\n",
    "        #print(f\"Training Set Values: {training_set_values}\")\n",
    "        \n",
    "        # Initialize the comparison results\n",
    "        binary_comparisons = {}\n",
    "\n",
    "        # Determine the signs of the coefficients using OLS (all data)\n",
    "        if ols_use_all_data:\n",
    "            ols_coefficients = model.get_ols_coefficients(True)\n",
    "            weight_signs = np.sign(list(ols_coefficients.values()))\n",
    "\n",
    "        # loop over the training set values\n",
    "        for training_set_index, training_set_value in enumerate(training_set_values):\n",
    "\n",
    "            # Timing\n",
    "            set_start_time = time.time()\n",
    "            print(f\"Training Set Value: {training_set_value} for dataset {dataset}\")\n",
    "\n",
    "            # Generate cross-validation indices\n",
    "            cv = model.cv_indexing(cv_folds, training_set_value)\n",
    "\n",
    "            # Initialize the comparison results for this training set value\n",
    "            binary_comparisons[training_set_value] = {}\n",
    "\n",
    "            # Vary eta values from 10^-4 to 10^5, including 0 and inf, and loop over them\n",
    "            etas = np.logspace(-4, 5, num=num_etas)\n",
    "            etas = np.insert(etas, 0, 0)\n",
    "            etas = np.append(etas, np.inf)\n",
    "            for eta in etas:\n",
    "\n",
    "                # Initialize the comparison results for this eta\n",
    "                binary_comparisons[training_set_value][eta] = []\n",
    "\n",
    "                # Crossvalidate over the folds for a specific eta\n",
    "                for i in range(cv_folds):\n",
    "\n",
    "                    # Print info about this iteration\n",
    "                    if verbose: print(f\"Training Set Value: {training_set_value}, Eta: {eta}, Fold: {i} of {range(cv_folds)}\")\n",
    "\n",
    "                    # Get training and test indices for the i-th fold\n",
    "                    train_indices = np.where(cv[i] == 1)[0]\n",
    "                    test_indices = np.where(cv[i] == 0)[0]\n",
    "                    \n",
    "                    # Split the data\n",
    "                    X_train, X_test = model.X.iloc[train_indices], model.X.iloc[test_indices]\n",
    "                    y_train, y_test = model.y.iloc[train_indices], model.y.iloc[test_indices]\n",
    "\n",
    "                    # Get OLS coefficients for the training data\n",
    "                    ols_coefficients = model.get_ols_coefficients(False, X_train, y_train)\n",
    "\n",
    "                    # Determine the signs of the coefficients using OLS on only the training data\n",
    "                    if not ols_use_all_data:\n",
    "                        weight_signs = np.sign(list(ols_coefficients.values()))\n",
    "\n",
    "                    '''\n",
    "                    # Run the half ridge regression with mcmc\n",
    "                    half_ridge_trace = half_ridge(X_train, y_train, model.ols_coefficients, prior_eta=eta)\n",
    "                    var_dict = {}\n",
    "                    for variable in half_ridge_trace.posterior:\n",
    "                        var_dict[variable] = half_ridge_trace.posterior[variable].values.flatten()\n",
    "                    # Results into a dataframe\n",
    "                    var_weights = pd.DataFrame(var_dict)\n",
    "                    # Means for all the weights\n",
    "                    var_means_array = var_weights.mean(axis=0)\n",
    "                    # Convert the dataframe var_means_array to a dictionary with the column names as keys\n",
    "                    half_ridge_coefficients = {col: var_means_array[col] for col in var_weights.columns}\n",
    "                    '''\n",
    "                    \n",
    "                    # Run the half ridge regression with rejection sampling\n",
    "                    start_time = time.time()\n",
    "                    half_ridge_coefficients = half_ridge_rejection_sampling(weight_signs, ols_coefficients, X_train, y_train, eta, chain_lengths[training_set_index])\n",
    "                    end_time = time.time()\n",
    "                    duration = end_time - start_time\n",
    "                    if verbose: print(f\"      Half ridge regression rejection sampling took {duration} seconds\")\n",
    "\n",
    "                    # Calculate the binary comparisons for the models\n",
    "                    sorted_half_ridge_coefficients = {k: half_ridge_coefficients[k] for k in sorted(half_ridge_coefficients)}\n",
    "                    sorted_half_ridge_coefficients['intercept'] = 0\n",
    "                    if verbose: print(f\"      half-ridge coefficients: {sorted_half_ridge_coefficients}\")\n",
    "                    binary_comparison = calculate_binary_comparison(sorted_half_ridge_coefficients, X_test, y_test)\n",
    "                    binary_comparisons[training_set_value][eta].append(binary_comparison)\n",
    "                    if verbose: print(f\"      binary comparison: {binary_comparison}\")\n",
    "\n",
    "                    '''\n",
    "                    # Calculate SSE for the half ridge coefficients\n",
    "                    half_ridge_sse = calculate_sse(sorted_half_ridge_coefficients, X_test, y_test)\n",
    "                    #binary_comparisons[training_set_value][eta].append(half_ridge_sse)\n",
    "                    print(f\"half ridge sse: {half_ridge_sse}\")\n",
    "                    '''\n",
    "\n",
    "            set_end_time = time.time()\n",
    "            set_duration = set_end_time - set_start_time\n",
    "            print(f\"   Training Set {training_set_value} full run took {set_duration} seconds\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Save the binary comparisons to a file\n",
    "        with open('model_results/ucirepo_' + str(dataset) + '_' + str(target) + '.csv', 'wb') as file:\n",
    "            pickle.dump(binary_comparisons, file)\n",
    "        \n",
    "        # increment the target\n",
    "        target += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists for dataset house.world. Skipping...\n",
      "Model already exists for dataset mortality. Skipping...\n",
      "Model already exists for dataset cit.world. Skipping...\n",
      "Model already exists for dataset prf.world. Skipping...\n",
      "Model already exists for dataset bodyfat.world. Skipping...\n",
      "Model already exists for dataset car.world. Skipping...\n",
      "Training Set Value: 10 for dataset cloud\n",
      "   Training Set 10 full run took 7.152557373046875e-06 seconds\n",
      "Training Set Value: 20 for dataset cloud\n",
      "   Training Set 20 full run took 1.0013580322265625e-05 seconds\n",
      "Training Set Value: 115 for dataset cloud\n",
      "   Training Set 115 full run took 6.9141387939453125e-06 seconds\n",
      "Training Set Value: 10 for dataset dropout\n",
      "   Training Set 10 full run took 1.8358230590820312e-05 seconds\n",
      "Training Set Value: 20 for dataset dropout\n",
      "   Training Set 20 full run took 1.71661376953125e-05 seconds\n",
      "Training Set Value: 115 for dataset dropout\n",
      "   Training Set 115 full run took 1.811981201171875e-05 seconds\n",
      "Training Set Value: 10 for dataset fat.world\n",
      "   Training Set 10 full run took 1.2874603271484375e-05 seconds\n",
      "Training Set Value: 20 for dataset fat.world\n",
      "   Training Set 20 full run took 1.1920928955078125e-05 seconds\n",
      "Training Set Value: 115 for dataset fat.world\n",
      "   Training Set 115 full run took 1.0967254638671875e-05 seconds\n",
      "Training Set Value: 10 for dataset fuel.world\n",
      "   Training Set 10 full run took 7.867813110351562e-06 seconds\n",
      "Training Set Value: 20 for dataset fuel.world\n",
      "   Training Set 20 full run took 6.9141387939453125e-06 seconds\n",
      "Training Set Value: 115 for dataset fuel.world\n",
      "   Training Set 115 full run took 7.867813110351562e-06 seconds\n",
      "Training Set Value: 10 for dataset glps\n",
      "   Training Set 10 full run took 8.821487426757812e-06 seconds\n",
      "Training Set Value: 20 for dataset glps\n",
      "   Training Set 20 full run took 8.106231689453125e-06 seconds\n",
      "Training Set Value: 115 for dataset glps\n",
      "   Training Set 115 full run took 9.298324584960938e-06 seconds\n",
      "Training Set Value: 10 for dataset homeless.world\n",
      "   Training Set 10 full run took 6.9141387939453125e-06 seconds\n",
      "Training Set Value: 20 for dataset homeless.world\n",
      "   Training Set 20 full run took 7.867813110351562e-06 seconds\n",
      "Training Set Value: 115 for dataset homeless.world\n",
      "   Training Set 115 full run took 6.9141387939453125e-06 seconds\n",
      "Training Set Value: 10 for dataset landrent.world\n",
      "   Training Set 10 full run took 4.76837158203125e-06 seconds\n",
      "Training Set Value: 20 for dataset landrent.world\n",
      "   Training Set 20 full run took 5.9604644775390625e-06 seconds\n",
      "Training Set Value: 115 for dataset landrent.world\n",
      "   Training Set 115 full run took 6.198883056640625e-06 seconds\n",
      "Training Set Value: 10 for dataset mammal.world\n",
      "   Training Set 10 full run took 1.0967254638671875e-05 seconds\n",
      "Training Set Value: 20 for dataset mammal.world\n",
      "   Training Set 20 full run took 1.0967254638671875e-05 seconds\n",
      "Training Set Value: 115 for dataset mammal.world\n",
      "   Training Set 115 full run took 1.1920928955078125e-05 seconds\n",
      "Training Set Value: 10 for dataset oxidants\n",
      "   Training Set 10 full run took 6.9141387939453125e-06 seconds\n",
      "Training Set Value: 20 for dataset oxidants\n",
      "   Training Set 20 full run took 6.9141387939453125e-06 seconds\n",
      "Training Set Value: 10 for dataset attractiveness.men\n",
      "   Training Set 10 full run took 5.9604644775390625e-06 seconds\n",
      "Training Set Value: 20 for dataset attractiveness.men\n",
      "   Training Set 20 full run took 4.76837158203125e-06 seconds\n",
      "Training Set Value: 115 for dataset attractiveness.men\n",
      "   Training Set 115 full run took 1.3113021850585938e-05 seconds\n",
      "Training Set Value: 10 for dataset attractiveness.women\n",
      "   Training Set 10 full run took 4.76837158203125e-06 seconds\n",
      "Training Set Value: 20 for dataset attractiveness.women\n",
      "   Training Set 20 full run took 5.245208740234375e-06 seconds\n",
      "Training Set Value: 115 for dataset attractiveness.women\n",
      "   Training Set 115 full run took 5.7220458984375e-06 seconds\n",
      "Training Set Value: 10 for dataset fish.fertility\n",
      "   Training Set 10 full run took 6.198883056640625e-06 seconds\n",
      "Training Set Value: 20 for dataset fish.fertility\n",
      "   Training Set 20 full run took 7.152557373046875e-06 seconds\n",
      "Training Set Value: 115 for dataset fish.fertility\n",
      "   Training Set 115 full run took 7.152557373046875e-06 seconds\n",
      "Training Set Value: 10 for dataset oxygen\n",
      "   Training Set 10 full run took 1.7881393432617188e-05 seconds\n",
      "Training Set Value: 20 for dataset oxygen\n",
      "   Training Set 20 full run took 7.152557373046875e-06 seconds\n",
      "Training Set Value: 10 for dataset ozone\n",
      "   Training Set 10 full run took 5.0067901611328125e-06 seconds\n",
      "Training Set Value: 20 for dataset ozone\n",
      "   Training Set 20 full run took 5.0067901611328125e-06 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the 2018 Paper datasets, normalize the data, and split into training, validation, and test sets\n",
    "\n",
    "training_set_sizes = [10, 20, 115] # Used for 2018 paper\n",
    "\n",
    "# Loop over each dataset, loaded from file and formatted\n",
    "for dataset in paper2018_datasets:\n",
    "    model = Model()\n",
    "    target = 0\n",
    "\n",
    "    # iterate over the target (while the file exists)\n",
    "    while os.path.isfile('2018paper_datasets/2018paper_' + str(dataset) + '_' + str(target) + '.csv'):\n",
    "\n",
    "        model.import_from_file('2018paper_datasets/2018paper_' + str(dataset) + '_' + str(target) + '.csv')\n",
    "        model.format_data(categorical_encoding, missing_data_handling)\n",
    "        model.create_paired_data()\n",
    "        training_set_values = model.set_up_training_values(training_set_sizes, .6)\n",
    "\n",
    "\n",
    "        # Check if a file exists for the dataset in the model_results folder\n",
    "        if os.path.isfile(f\"model_results/2018paper_{dataset}_{target}.csv\") and not overwrite_model:\n",
    "            print(f\"Model already exists for dataset {dataset}. Skipping...\")\n",
    "            # increment the target\n",
    "            target += 1\n",
    "            continue\n",
    "\n",
    "        # Print the training set values for this dataset\n",
    "        #print(f\"Training Set Values: {training_set_values}\")\n",
    "        \n",
    "        # Initialize the comparison results\n",
    "        binary_comparisons = {}\n",
    "\n",
    "        # Determine the signs of the coefficients using OLS (all data)\n",
    "        if ols_use_all_data:\n",
    "            ols_coefficients = model.get_ols_coefficients(True)\n",
    "            weight_signs = np.sign(list(ols_coefficients.values()))\n",
    "\n",
    "        # loop over the training set values\n",
    "        for training_set_index, training_set_value in enumerate(training_set_values):\n",
    "\n",
    "            # Timing\n",
    "            set_start_time = time.time()\n",
    "            print(f\"Training Set Value: {training_set_value} for dataset {dataset}\")\n",
    "\n",
    "            # Generate cross-validation indices\n",
    "            cv = model.cv_indexing(cv_folds, training_set_value)\n",
    "\n",
    "            # Initialize the comparison results for this training set value\n",
    "            binary_comparisons[training_set_value] = {}\n",
    "\n",
    "            # Vary eta values from 10^-4 to 10^5, including 0 and inf, and loop over them\n",
    "            etas = np.logspace(-4, 5, num=num_etas)\n",
    "            etas = np.insert(etas, 0, 0)\n",
    "            etas = np.append(etas, np.inf)\n",
    "            for eta in etas:\n",
    "\n",
    "                # Initialize the comparison results for this eta\n",
    "                binary_comparisons[training_set_value][eta] = []\n",
    "\n",
    "                # Crossvalidate over the folds for a specific eta\n",
    "                for i in range(cv_folds):\n",
    "\n",
    "                    # Print info about this iteration\n",
    "                    if verbose: print(f\"Training Set Value: {training_set_value}, Eta: {eta}, Fold: {i} of {range(cv_folds)}\")\n",
    "\n",
    "                    # Get training and test indices for the i-th fold\n",
    "                    train_indices = np.where(cv[i] == 1)[0]\n",
    "                    test_indices = np.where(cv[i] == 0)[0]\n",
    "                    \n",
    "                    # Split the data\n",
    "                    X_train, X_test = model.X.iloc[train_indices], model.X.iloc[test_indices]\n",
    "                    y_train, y_test = model.y.iloc[train_indices], model.y.iloc[test_indices]\n",
    "\n",
    "                    # Get OLS coefficients for the training data\n",
    "                    ols_coefficients = model.get_ols_coefficients(False, X_train, y_train)\n",
    "\n",
    "                    # Determine the signs of the coefficients using OLS on only the training data\n",
    "                    if not ols_use_all_data:\n",
    "                        weight_signs = np.sign(list(ols_coefficients.values()))\n",
    "\n",
    "                    '''\n",
    "                    # Run the half ridge regression with mcmc\n",
    "                    half_ridge_trace = half_ridge(X_train, y_train, model.ols_coefficients, prior_eta=eta)\n",
    "                    var_dict = {}\n",
    "                    for variable in half_ridge_trace.posterior:\n",
    "                        var_dict[variable] = half_ridge_trace.posterior[variable].values.flatten()\n",
    "                    # Results into a dataframe\n",
    "                    var_weights = pd.DataFrame(var_dict)\n",
    "                    # Means for all the weights\n",
    "                    var_means_array = var_weights.mean(axis=0)\n",
    "                    # Convert the dataframe var_means_array to a dictionary with the column names as keys\n",
    "                    half_ridge_coefficients = {col: var_means_array[col] for col in var_weights.columns}\n",
    "                    '''\n",
    "                    \n",
    "                    # Run the half ridge regression with rejection sampling\n",
    "                    start_time = time.time()\n",
    "                    half_ridge_coefficients = half_ridge_rejection_sampling(weight_signs, ols_coefficients, X_train, y_train, eta, chain_lengths[training_set_index])\n",
    "                    end_time = time.time()\n",
    "                    duration = end_time - start_time\n",
    "                    if verbose: print(f\"      Half ridge regression rejection sampling took {duration} seconds\")\n",
    "\n",
    "                    # Calculate the binary comparisons for the models\n",
    "                    sorted_half_ridge_coefficients = {k: half_ridge_coefficients[k] for k in sorted(half_ridge_coefficients)}\n",
    "                    sorted_half_ridge_coefficients['intercept'] = 0\n",
    "                    if verbose: print(f\"      half-ridge coefficients: {sorted_half_ridge_coefficients}\")\n",
    "                    #binary_comparison = calculate_binary_comparison(sorted_half_ridge_coefficients, X_test, y_test)\n",
    "                    binary_comparison = calculate_accuracy(sorted_half_ridge_coefficients, X_test, y_test, True)\n",
    "                    binary_comparisons[training_set_value][eta].append(binary_comparison)\n",
    "                    if verbose: print(f\"      binary comparison: {binary_comparison}\")\n",
    "\n",
    "                    '''\n",
    "                    # Calculate SSE for the half ridge coefficients\n",
    "                    half_ridge_sse = calculate_sse(sorted_half_ridge_coefficients, X_test, y_test)\n",
    "                    #binary_comparisons[training_set_value][eta].append(half_ridge_sse)\n",
    "                    print(f\"half ridge sse: {half_ridge_sse}\")\n",
    "                    '''\n",
    "\n",
    "            set_end_time = time.time()\n",
    "            set_duration = end_time - start_time\n",
    "            print(f\"   Training Set {training_set_value} full run took {set_duration} seconds\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Save the binary comparisons to a file\n",
    "        with open('model_results/2018paper_' + str(dataset) + '_' + str(target) + '.csv', 'wb') as file:\n",
    "            pickle.dump(binary_comparisons, file)\n",
    "        \n",
    "        # increment the target\n",
    "        target += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
